\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{soul}      % strikethrough (\st) support for pandoc >= 3.0.0
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{Machine Learning}
    \author{Bach Van Hoang Bao}
    \date{\today}
    
    % --- Custom styles ---
	\setcounter{tocdepth}{3}
	\setcounter{secnumdepth}{3}

	\makeatletter
	\edef\mytitle{\@title}
	\makeatother

	\makeatletter
	\edef\myself{\@author}
	\makeatother
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@ges}{\let\PY@bf=\textbf\let\PY@it=\textit}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
\begin{titlepage}
    \newgeometry{top=1in,bottom=1in,right=0.5in,left=0.5in}
    
    \begin{center}

        \begin{figure}[!htb]
            \centering
            \includegraphics[width=0.3\textwidth]{exam4_3_files/cqf_logo.png}
        \end{figure}
     
        \huge \textbf{CQF Exam Three}\\
        \vspace{0.5cm}
        \Large \textbf{\mytitle}\\
        \vspace{0.7cm}
        \large \textbf{\myself}\\
        \vspace{0.2cm}
        \textbf{June 2023 cohort}\\ 

        \mbox{}
        \vfill

        \begin{figure}[!htb]
            \centering
            \includegraphics[width=0.2\textwidth]{exam4_3_files/FitchLearning_logo.png}
        \end{figure}

        \today
    \end{center}

\end{titlepage} 
    
    

    
    \hypertarget{what-are-voting-classifiers-in-ensemble-learning}{%
\section{What are voting classifiers in ensemble
learning?}\label{what-are-voting-classifiers-in-ensemble-learning}}

    Voting classifiers are based on the idea of aggregating the predictions
of multiple classifiers to make a final decision {[}1{]}. There are two
main types of voting classifiers:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Majority Class Labels (Majority/Hard Voting): In majority voting, the
  predicted class label for a particular sample is the class label that
  represents the majority (mode) of the class labels predicted by each
  individual classifier.

  E.g., if the prediction for a given sample is

  \begin{itemize}
  \tightlist
  \item
    classifier 1 $\rightarrow$ class 1
  \item
    classifier 2 $\rightarrow$ class 1
  \item
    classifier 3 $\rightarrow$ class 2
  \end{itemize}

  The VotingClassifier (with voting = 'hard') would classify the sample as  "class 1" based on the majority class label.

  In the cases of a tie, the VotingClassifier will select the class
  based on the ascending sort order. E.g., in the following scenario

  \begin{itemize}
  \tightlist
  \item
    classifier 1 $\rightarrow$ class 2
  \item
    classifier 2 $\rightarrow$ class 1
  \end{itemize}

  The class label 1 will be assigned to the sample.
  
\item
  Soft Voting Classifier: In contrast to majority voting (hard voting),
  soft voting returns the class label as argmax of the sum of predicted
  probabilities.

  Specific weights can be assigned to each classifier via the weights
  parameter. When weights are provided, the predicted class
  probabilities for each classifier are collected, multiplied by the
  classifier weight, and averaged. The final class label is then derived
  from the class label with the highest average probability.

  To illustrate this with a simple example, let's assume we have 3
  classifiers and a 3-class classification problems where we assign
  equal weights to all classifiers: $w_1 = 1, w_2 = 1, w_3 = 1$.

  The weighted average probabilities for a sample would then be
  calculated as follows:

\begin{tabular}{|c|c|c|c|}
\hline
Classifier & Class 1 & Class 2 & Class 3 \\
\hline
Classifier 1 & $w_1 \times 0.2$ & $w_1 \times 0.5$ & $w_1 \times 0.3$ \\
Classifier 2 & $w_2 \times 0.6$ & $w_2 \times 0.3$ & $w_2 \times 0.1$ \\
Classifier 3 & $w_3 \times 0.3$ & $w_3 \times 0.4$ & $w_3 \times 0.3$ \\
\hline
Weighted Avg & 0.37 & 0.4 & 0.23 \\
\hline
\end{tabular}


Here, the predicted class label is 2, since it has the highest average probability.
\end{enumerate}


\pagebreak
    \hypertarget{explain-the-role-of-the-regularization-parameter-c-in-a-support-vector-machine-svm-model.-how-does-varying-c-affect-the-models-bias-and-variance-trade-off}{%
\section{\texorpdfstring{Explain the role of the regularization
parameter \(C\) in a Support Vector Machine (SVM) model. How does
varying \(C\) affect the model's bias and variance
trade-off?}{Explain the role of the regularization parameter C in a Support Vector Machine (SVM) model. How does varying C affect the model's bias and variance trade-off?}}\label{explain-the-role-of-the-regularization-parameter-c-in-a-support-vector-machine-svm-model.-how-does-varying-c-affect-the-models-bias-and-variance-trade-off}}

    Consider the mathematical equation for the solf margin of non linearly
separable data {[}2{]} \(\mathbf{x}_n\), \(y_n\):
\begin{align*}
\min_{\boldsymbol{w},b,\boldsymbol{\xi}} \quad & \frac{1}{2}\|{\boldsymbol{w}}\|^2 + C\sum_{n=1}^{N}\xi_{n} \\
\text{subject to} \quad & y_{n}(\langle{\boldsymbol{w}},{\boldsymbol{x}}_{n}\rangle+b) \geqslant 1 - \xi_{n} \\
& \xi_{n} \geqslant 0
\end{align*}
    Where: 
    \begin{itemize}
  	\tightlist
  	\item
    	\(\boldsymbol{w}\) is the normal vector of the hyper plane
    \item
    	\(\lVert{\boldsymbol{w}}\rVert^2\) is the regularizer
  	\item
    	\(\boldsymbol{x}_n\) is the feature vector \(n^{th}\) 
  	\item
    	\(y_n\) is the label \(n^{th}\) 
   	\item
   		\(\xi_n\) is the slack term that measures the distance of a positive 		example \(x_{+}\) to the positive margin hyperplane \((\langle{\mathbf{w}, \mathbf{x}}\rangle + b = 1)\) when \(x_{+}\) is on the wrong side.
  	\end{itemize}

    The parameter \(C > 0\) trades off the size of the margin and the total
amount of slack that we have. A large value of \(C\) implies low
regularization, as we give the slack variables larger weight, hence
giving more priority to examples that do not lie on the correct side of
the margin.

    Let take a look at the impact of \(C\) in the model using the breast
cancer data from \texttt{Scikit-Learn} library.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd} 
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np} 
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k+kn}{import} \PY{n}{classification\PYZus{}report}\PY{p}{,} \PY{n}{confusion\PYZus{}matrix} 
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{datasets} \PY{k+kn}{import} \PY{n}{load\PYZus{}breast\PYZus{}cancer} 
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{svm} \PY{k+kn}{import} \PY{n}{SVC} 
\PY{k+kn}{import} \PY{n+nn}{warnings}
\PY{n}{warnings}\PY{o}{.}\PY{n}{filterwarnings}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ignore}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{cancer} \PY{o}{=} \PY{n}{load\PYZus{}breast\PYZus{}cancer}\PY{p}{(}\PY{p}{)} 

\PY{c+c1}{\PYZsh{} The data set is presented in a dictionary form: }
\PY{n+nb}{print}\PY{p}{(}\PY{n}{cancer}\PY{o}{.}\PY{n}{keys}\PY{p}{(}\PY{p}{)}\PY{p}{)} 
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
dict\_keys(['data', 'target', 'frame', 'target\_names', 'DESCR', 'feature\_names',
'filename', 'data\_module'])
    \end{Verbatim}
    
\pagebreak

    Now we will extract all features into the new data frame and our target
features into separate data frames.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{df\PYZus{}feat} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{cancer}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{columns} \PY{o}{=} \PY{n}{cancer}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{feature\PYZus{}names}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)} 
\PY{c+c1}{\PYZsh{} cancer column is our target }
\PY{n}{df\PYZus{}target} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{cancer}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{columns} \PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Cancer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)} 
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Feature Variables: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} 
\PY{n+nb}{print}\PY{p}{(}\PY{n}{df\PYZus{}feat}\PY{o}{.}\PY{n}{info}\PY{p}{(}\PY{p}{)}\PY{p}{)} 
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Feature Variables:
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 569 entries, 0 to 568
Data columns (total 30 columns):
 \#   Column                   Non-Null Count  Dtype
---  ------                   --------------  -----
 0   mean radius              569 non-null    float64
 1   mean texture             569 non-null    float64
 2   mean perimeter           569 non-null    float64
 3   mean area                569 non-null    float64
 4   mean smoothness          569 non-null    float64
 5   mean compactness         569 non-null    float64
 6   mean concavity           569 non-null    float64
 7   mean concave points      569 non-null    float64
 8   mean symmetry            569 non-null    float64
 9   mean fractal dimension   569 non-null    float64
 10  radius error             569 non-null    float64
 11  texture error            569 non-null    float64
 12  perimeter error          569 non-null    float64
 13  area error               569 non-null    float64
 14  smoothness error         569 non-null    float64
 15  compactness error        569 non-null    float64
 16  concavity error          569 non-null    float64
 17  concave points error     569 non-null    float64
 18  symmetry error           569 non-null    float64
 19  fractal dimension error  569 non-null    float64
 20  worst radius             569 non-null    float64
 21  worst texture            569 non-null    float64
 22  worst perimeter          569 non-null    float64
 23  worst area               569 non-null    float64
 24  worst smoothness         569 non-null    float64
 25  worst compactness        569 non-null    float64
 26  worst concavity          569 non-null    float64
 27  worst concave points     569 non-null    float64
 28  worst symmetry           569 non-null    float64
 29  worst fractal dimension  569 non-null    float64
dtypes: float64(30)
memory usage: 133.5 KB
None
    \end{Verbatim}

\pagebreak

    We will split the training data and the test data using 70:30 ratio

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{train\PYZus{}test\PYZus{}split} 

\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(} \PY{n}{df\PYZus{}feat}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{n}{df\PYZus{}target}\PY{p}{)}\PY{p}{,} 
				\PY{n}{test\PYZus{}size} \PY{o}{=} \PY{l+m+mf}{0.30}\PY{p}{,} \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{101}\PY{p}{)} 
\end{Verbatim}
\end{tcolorbox}

    And fit the data to our SVC model. After that we can see the
hyperparameters of our model using \texttt{get\_param()} method.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} train the model on train set }
\PY{n}{model} \PY{o}{=} \PY{n}{SVC}\PY{p}{(}\PY{p}{)} 
\PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)} 
\end{Verbatim}
\end{tcolorbox}

        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model}\PY{o}{.}\PY{n}{get\PYZus{}params}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\{'C': 1.0,
 'break\_ties': False,
 'cache\_size': 200,
 'class\_weight': None,
 'coef0': 0.0,
 'decision\_function\_shape': 'ovr',
 'degree': 3,
 'gamma': 'scale',
 'kernel': 'rbf',
 'max\_iter': -1,
 'probability': False,
 'random\_state': None,
 'shrinking': True,
 'tol': 0.001,
 'verbose': False\}
\end{Verbatim}
\end{tcolorbox}
        
    The default value of \(C\) is 1. Now we can observe the model performace
using the confusion matrix.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} print prediction results }
\PY{n}{predictions} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)} 
\PY{n+nb}{print}\PY{p}{(}\PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{predictions}\PY{p}{)}\PY{p}{)} 
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
              precision    recall  f1-score   support

           0       0.95      0.85      0.90        66
           1       0.91      0.97      0.94       105

    accuracy                           0.92       171
   macro avg       0.93      0.91      0.92       171
weighted avg       0.93      0.92      0.92       171

    \end{Verbatim}

    Let's change the value of \(C=0.001\) and see the result.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} train the model on train set }
\PY{n}{model} \PY{o}{=} \PY{n}{SVC}\PY{p}{(}\PY{n}{C}\PY{o}{=}\PY{l+m+mf}{0.001}\PY{p}{)} 
\PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)} 
\PY{c+c1}{\PYZsh{} print prediction results }
\PY{n}{predictions} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)} 
\PY{n+nb}{print}\PY{p}{(}\PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{predictions}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        66
           1       0.61      1.00      0.76       105

    accuracy                           0.61       171
   macro avg       0.31      0.50      0.38       171
weighted avg       0.38      0.61      0.47       171

    \end{Verbatim}

    \hypertarget{conclusion}{%
\subsection{Conclusion}\label{conclusion}}

    When \(C\) is set to a small value (e.g., close to 0), the SVM places a
higher emphasis on maximizing the margin and finding the hyperplane that
separates the data points with as few errors as possible. In this case,
the model is more tolerant of misclassifications (training errors) and
is willing to accept a wider margin with a few support vectors. The
model's bias is higher, as it tends to underfit the training data by
allowing more training errors, but the variance is lower because it
maintains a simpler decision boundary.

When \(C\) is set to a large value, the SVM imposes a stronger penalty
on misclassified points and strives to minimize training errors, even if
it means having a narrower margin and more support vectors. The model's
bias is lower because it tries to fit the training data as closely as
possible, potentially leading to a more complex decision boundary.
However, the variance is higher because the model is sensitive to
individual data points, which can result in overfitting.

\pagebreak

    \hypertarget{produce-a-model-to-predict-positive-moves-up-trend-using-machine-learning-model.}{%
\section{Produce a model to predict positive moves (up trend) using
machine learning
model.}\label{produce-a-model-to-predict-positive-moves-up-trend-using-machine-learning-model.}}

    In this section, we will create a Machine Learning (ML) model to predict
positive movements using raw financial data from Yahoo Finance. The goal is to emphasize the work flow structure and develop a comprehensive framework from ideation to model evaluation, applying techniques and knowledge from the CQF module 4.

    The 7 steps of ML work flow {[}3{]}:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.0455}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2841}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.6705}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Workflow
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Remark
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & Ideation & Predict positive moves from the given dataset \\
2 & Data Collection & Download the data from Yahoo Finance and store the
data set \\
3 & Exploratory Data Analysis & Study summary statistics \\
4 & Cleaning Dataset & Trying to resolve the missing data \\
5 & Transformation & Perform feature scaling based on EDA \\
6 & Modeling & Building and training classification model \\
7 & Metrics & Validating the model performance \\
\end{longtable}

    \hypertarget{step-1-ideation}{%
\subsection{STEP 1: Ideation}\label{step-1-ideation}}

    The objective of the exam is to create a model for predicting upward and
downward movements of the underlying asset. This is a classification
problem, so we will approach it by assigning a classification label
\texttt{{[}0{]}} for a downward trend and \texttt{{[}1{]}} for an upward
trend. We will use the \texttt{SPY} ticker as an example and utilize
data from \texttt{2008-10-16} to \texttt{2023-10-16}. The reason for
choosing this date range is that it encompasses various financial
regimes, including the 2008 financial crash and the 2020 COVID-19
recession.

Given that we are working with financial time series data, the work flow is relatively straightforward. After downloading and storing the data, we will begin by exploring the data to identify any meaningful structures or trends. Next, we'll create the classification labels and address class imbalance. We will apply feature engineering techniques to generate new features from the original data and select the most crucial features for our model. The data will be saved under \texttt{../SPY1D.csv}.

Subsequently, we will split the data into training and testing sets, fitting and transforming the training set, and then transforming the test set. This ensures that we avoid any data leakage issues. We will explore the training set to identify trends and significant structures in the data. If necessary, we will scale the data and use the transformed data to train our model.

During the model training process, we will compare the cross-validation accuracy of multiple classification algorithms with default parameters and select the most promising candidate for further tuning. Hyper parameters will be tuned to optimize the selected candidate, leading to the creation of a final model. This final model will be saved as \texttt{final\_model.joblib} for future use.

In terms of performance measurement, we will employ the confusion matrix and AUC-ROC curve to evaluate our model's performance and document the entire process.

\pagebreak

    \hypertarget{step-2-data-collection}{%
\subsection{STEP 2: Data Collection}\label{step-2-data-collection}}

    We will use the \texttt{yfinance} package to download daily trading data
from Yahoo Finance. The recommended data should span a 5-year period,
which is considered sufficient. The downloaded data will be saved in the
\texttt{.csv} format and can be accessed later using the file name
\texttt{SPY1D.csv}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Download data for TSLA and store as csv file}
\PY{n}{spy} \PY{o}{=} \PY{n}{yf}\PY{o}{.}\PY{n}{download}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{SPY}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{start} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2008\PYZhy{}10\PYZhy{}16}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{end} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2023\PYZhy{}10\PYZhy{}16}\PY{l+s+s1}{\PYZsq{}} \PY{p}{,} \PY{n}{interval}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{1D}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{spy}\PY{o}{.}\PY{n}{to\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SPY1D.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[*********************100\%\%**********************]  1 of 1 completed
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{spy} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../module\PYZus{}4/SPY1D.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Verify the downloaded data}
\PY{n}{spy}\PY{o}{.}\PY{n}{info}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 3774 entries, 0 to 3773
Data columns (total 7 columns):
 \#   Column     Non-Null Count  Dtype
---  ------     --------------  -----
 0   Date       3774 non-null   object
 1   Open       3774 non-null   float64
 2   High       3774 non-null   float64
 3   Low        3774 non-null   float64
 4   Close      3774 non-null   float64
 5   Adj Close  3774 non-null   float64
 6   Volume     3774 non-null   int64
dtypes: float64(5), int64(1), object(1)
memory usage: 206.5+ KB
    \end{Verbatim}

    \hypertarget{step-3-eda}{%
\subsection{STEP 3: EDA}\label{step-3-eda}}

    Visualize asset path:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{plotly}\PY{n+nn}{.}\PY{n+nn}{express} \PY{k}{as} \PY{n+nn}{px}

\PY{n}{fig} \PY{o}{=} \PY{n}{px}\PY{o}{.}\PY{n}{line}\PY{p}{(}\PY{n}{spy}\PY{p}{,} \PY{n}{x}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Adj Close}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{labels} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Adj Close}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Close Price (USD)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{\PYZcb{}}\PY{p}{,} \PY{n}{title} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{S\PYZam{}P 500 ETF Trust (SPY) Daily}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{fig}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}\PY{p}{;}
\end{Verbatim}
\end{tcolorbox}

    
    
    \hypertarget{calculate-returns}{%
\subsubsection{Calculate returns}\label{calculate-returns}}

    We can plot the distribution of returns and the closing price movement
to identify any trends or significant information regarding the returns
that could be useful.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{spy}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Returns}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{spy}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Adj Close}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{diff}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{stats} \PY{k+kn}{import} \PY{n}{norm}

\PY{c+c1}{\PYZsh{} Plot the return histogram}
\PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{15}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{)}\PY{p}{)}
\PY{n}{ax1} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{spy}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Returns}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{bins}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{ax1}\PY{p}{)}
\PY{n}{ax1}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Return}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{ax1}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{ax1}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Return distribution}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Plot the normal distribution}
\PY{n}{mu} \PY{o}{=} \PY{n}{spy}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Returns}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
\PY{n}{sigma} \PY{o}{=} \PY{n}{spy}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Returns}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{p}{)}
\PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{mu} \PY{o}{\PYZhy{}} \PY{l+m+mi}{3}\PY{o}{*}\PY{n}{sigma}\PY{p}{,} \PY{n}{mu} \PY{o}{+} \PY{l+m+mi}{3}\PY{o}{*}\PY{n}{sigma}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{norm}\PY{o}{.}\PY{n}{pdf}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{mu}\PY{p}{,} \PY{n}{sigma}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{exam4_3_files/exam4_3_39_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The return is definately not normally distributed. There is a high peak
and very fat tails.

    \hypertarget{feature-specify}{%
\subsubsection{Feature Specify}\label{feature-specify}}

    Using the feature list table from the exam, we will generate features
based on the historical data we have acquired. Additionally, I've
included 10 lagged prices in the feature list, operating on the
assumption that historical data may possess predictive capabilities.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{15}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Create features (predictors) list}
\PY{n}{features\PYZus{}list} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{c+c1}{\PYZsh{} Intraday price range}
\PY{n}{spy}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{OC}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{spy}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Open}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{spy}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Close}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\PY{n}{spy}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{HL}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{spy}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{High}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{spy}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Low}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\PY{c+c1}{\PYZsh{} Sign of return or momentum}
\PY{n}{spy}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Sign}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sign}\PY{p}{(}\PY{n}{spy}\PY{o}{.}\PY{n}{Returns}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Append feature list}
\PY{n}{features\PYZus{}list}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{OC}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{features\PYZus{}list}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{HL}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{features\PYZus{}list}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Sign}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Pass Returns, Volatility}
\PY{k}{for} \PY{n}{r} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{65}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{:}
    \PY{n}{spy}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Ret\PYZus{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{r}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{spy}\PY{o}{.}\PY{n}{Returns}\PY{o}{.}\PY{n}{rolling}\PY{p}{(}\PY{n}{r}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
    \PY{n}{spy}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Std\PYZus{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{r}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{spy}\PY{o}{.}\PY{n}{Returns}\PY{o}{.}\PY{n}{rolling}\PY{p}{(}\PY{n}{r}\PY{p}{)}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{p}{)}
    \PY{n}{features\PYZus{}list}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Ret\PYZus{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{r}\PY{p}{)}\PY{p}{)}
    \PY{n}{features\PYZus{}list}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Std\PYZus{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{r}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} SMA and EMA}
\PY{k}{for} \PY{n}{a} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{200}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{:}
    \PY{n}{spy}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SMA\PYZus{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{r}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{spy}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Adj Close}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{rolling}\PY{p}{(}\PY{n}{r}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
    \PY{n}{spy}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{EMA\PYZus{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{a}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{spy}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Adj Close}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{ewm}\PY{p}{(}\PY{n}{span} \PY{o}{=} \PY{n}{a}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
    \PY{n}{features\PYZus{}list}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SMA\PYZus{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{r}\PY{p}{)}\PY{p}{)}
    \PY{n}{features\PYZus{}list}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{EMA\PYZus{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{r}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Lag price}
\PY{k}{for} \PY{n}{lag} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{:}
    \PY{n}{spy}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lag\PYZus{}}\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{lag}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{spy}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Adj Close}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{shift}\PY{p}{(}\PY{n}{lag}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Drop NaN values}
\PY{n}{spy}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{define-target}{%
\subsubsection{Define target}\label{define-target}}

    We define the target variable to be whether the `SPY' price will close
up or down on the next trading day. If tomorrow's closing price is
greater than today's closing price by at least 5\%, we consider the
asset to be ``up''; otherwise, it is considered ``down.''

We assign a value of \texttt{1} to denote an ``up'' move and \texttt{0}
to represent a ``down'' move for the target variable. This target
variable can be described as follows:

    \[
y_t = 
\begin{cases} 
1, & \text{If} \quad p_{t} < 0.995 \times p_{t+1}  \\ 
0, & \text{Otherwise}
\end{cases}
\]

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{16}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Define Target}
\PY{n}{spy}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{spy}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Adj Close}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{shift}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{l+m+mf}{0.995} \PY{o}{*} \PY{n}{spy}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Adj Close}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Check output}
\PY{n}{spy}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}
        
    I am going to split the data into the \texttt{train\_set} and
\texttt{test\_set} and perform exploratory data analysis (EDA) and data
cleaning exclusively on the \texttt{train\_set} to prevent any potential
data leakage from the EDA process.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{17}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Copy the original data}
\PY{n}{data} \PY{o}{=} \PY{n}{spy}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}index}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Specify the features matrix `X`}
\PY{n}{X} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Open}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Close}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{High}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Low}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Adj Close}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Returns}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Volume}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{X}\PY{o}{.}\PY{n}{info}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
<class 'pandas.core.frame.DataFrame'>
Index: 3714 entries, 2009-01-13 to 2023-10-13
Data columns (total 53 columns):
 \#   Column   Non-Null Count  Dtype
---  ------   --------------  -----
 0   OC       3714 non-null   float64
 1   HL       3714 non-null   float64
 2   Sign     3714 non-null   float64
 3   Ret\_10   3714 non-null   float64
 4   Std\_10   3714 non-null   float64
 5   Ret\_15   3714 non-null   float64
 6   Std\_15   3714 non-null   float64
 7   Ret\_20   3714 non-null   float64
 8   Std\_20   3714 non-null   float64
 9   Ret\_25   3714 non-null   float64
 10  Std\_25   3714 non-null   float64
 11  Ret\_30   3714 non-null   float64
 12  Std\_30   3714 non-null   float64
 13  Ret\_35   3714 non-null   float64
 14  Std\_35   3714 non-null   float64
 15  Ret\_40   3714 non-null   float64
 16  Std\_40   3714 non-null   float64
 17  Ret\_45   3714 non-null   float64
 18  Std\_45   3714 non-null   float64
 19  Ret\_50   3714 non-null   float64
 20  Std\_50   3714 non-null   float64
 21  Ret\_55   3714 non-null   float64
 22  Std\_55   3714 non-null   float64
 23  Ret\_60   3714 non-null   float64
 24  Std\_60   3714 non-null   float64
 25  SMA\_60   3714 non-null   float64
 26  EMA\_20   3714 non-null   float64
 27  EMA\_30   3714 non-null   float64
 28  EMA\_40   3714 non-null   float64
 29  EMA\_50   3714 non-null   float64
 30  EMA\_60   3714 non-null   float64
 31  EMA\_70   3714 non-null   float64
 32  EMA\_80   3714 non-null   float64
 33  EMA\_90   3714 non-null   float64
 34  EMA\_100  3714 non-null   float64
 35  EMA\_110  3714 non-null   float64
 36  EMA\_120  3714 non-null   float64
 37  EMA\_130  3714 non-null   float64
 38  EMA\_140  3714 non-null   float64
 39  EMA\_150  3714 non-null   float64
 40  EMA\_160  3714 non-null   float64
 41  EMA\_170  3714 non-null   float64
 42  EMA\_180  3714 non-null   float64
 43  EMA\_190  3714 non-null   float64
 44  lag\_1    3714 non-null   float64
 45  lag\_2    3714 non-null   float64
 46  lag\_3    3714 non-null   float64
 47  lag\_4    3714 non-null   float64
 48  lag\_5    3714 non-null   float64
 49  lag\_6    3714 non-null   float64
 50  lag\_7    3714 non-null   float64
 51  lag\_8    3714 non-null   float64
 52  lag\_9    3714 non-null   float64
dtypes: float64(53)
memory usage: 1.5+ MB
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{19}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Define label or target vector `y`}
\PY{n}{y} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\PY{n}{y}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{19}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
Date
2009-01-13    0
2009-01-14    1
2009-01-15    1
2009-01-16    0
2009-01-20    1
             ..
2023-10-09    1
2023-10-10    1
2023-10-11    0
2023-10-12    1
2023-10-13    0
Name: Target, Length: 3714, dtype: int64
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{20}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Splitting the datasets into training and testing data.}
\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Output the train and test data size}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Train and Test Size }\PY{l+s+si}{\PYZob{}}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, }\PY{l+s+si}{\PYZob{}}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Train and Test Size 2971, 743
    \end{Verbatim}

    \hypertarget{imbalance-class}{%
\subsubsection{Imbalance class}\label{imbalance-class}}

    Since this is a classification problem, it's important to check for any
imbalances in our labels.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{21}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} class frequency}
\PY{n}{c} \PY{o}{=} \PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}
\PY{n}{c}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{21}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
1    2361
0     610
Name: Target, dtype: int64
\end{Verbatim}
\end{tcolorbox}
        
    The label is imbalanced. We will create a weight function and
subsequently use it to address our problem when building a model.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{22}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} class weight function}
\PY{k}{def} \PY{n+nf}{cwts}\PY{p}{(}\PY{n}{label}\PY{p}{)}\PY{p}{:}
    \PY{n}{c0}\PY{p}{,} \PY{n}{c1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{bincount}\PY{p}{(}\PY{n}{label}\PY{p}{)}
    \PY{n}{w0}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{/}\PY{n}{c0}\PY{p}{)}\PY{o}{*}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{label}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{2} 
    \PY{n}{w1}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{/}\PY{n}{c1}\PY{p}{)}\PY{o}{*}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{label}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{2} 
    \PY{k}{return} \PY{p}{\PYZob{}}\PY{l+m+mi}{0}\PY{p}{:} \PY{n}{w0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:} \PY{n}{w1}\PY{p}{\PYZcb{}}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{23}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} check class weights}
\PY{n}{class\PYZus{}weight} \PY{o}{=} \PY{n}{cwts}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{)}
\PY{n}{class\PYZus{}weight}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{23}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\{0: 2.435245901639344, 1: 0.6291825497670478\}
\end{Verbatim}
\end{tcolorbox}
        
    \hypertarget{multi-collinearity-features}{%
\subsubsection{Multi collinearity
features}\label{multi-collinearity-features}}

    Collinear features can adversely affect our model's performance. We will
create a function to help us identify and drop these features, and then
apply it to our test dataset. Let's also visualize our correlation
matrix using the \texttt{sns.heatmap()} method.

\pagebreak
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{24}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{25}\PY{p}{,} \PY{l+m+mi}{22}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Identify features that are highly correlated}
\PY{n}{sns}\PY{o}{.}\PY{n}{heatmap}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)}\PY{o}{\PYZgt{}}\PY{l+m+mf}{0.9}\PY{p}{,}
            \PY{n}{annot}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
            \PY{n}{annot\PYZus{}kws}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{size}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+m+mi}{8}\PY{p}{\PYZcb{}}\PY{p}{,}
            \PY{n}{fmt}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{.2f}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
            \PY{n}{linewidth}\PY{o}{=}\PY{l+m+mf}{.5}\PY{p}{,}
            \PY{n}{cmap}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{coolwarm}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
            \PY{n}{cbar}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{;} \PY{c+c1}{\PYZsh{}cmap=\PYZdq{}crest\PYZdq{}, virids, magma}

\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Features Set Correlations}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{exam4_3_files/exam4_3_61_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
\pagebreak
    
    Feature scaling is also a crucial factor in our model's accuracy. We
need to scale the data before inputting it into our learning algorithm.
We can easily identify features that require scaling by using the
\texttt{sns.boxplot()} method.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{25}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} study the distribution}
\PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{14}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
\PY{n}{sns}\PY{o}{.}\PY{n}{boxplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{variable}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{data}\PY{o}{=}\PY{n}{pd}\PY{o}{.}\PY{n}{melt}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Boxplot of Features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{exam4_3_files/exam4_3_63_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Alternatively, we can identify features that require scaling by using
the \texttt{pd.describe()} method.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{26}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{26}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                OC           HL         Sign       Ret\_10       Std\_10  \textbackslash{}
count  2971.000000  2971.000000  2971.000000  2971.000000  2971.000000
mean     -0.032289     2.180993     0.119488     0.005261     0.009328
std       1.653484     1.984382     0.991306     0.033085     0.006957
min     -11.680008     0.299995    -1.000000    -0.265117     0.001264
25\%      -0.750000     1.129997    -1.000000    -0.007946     0.005041
50\%      -0.110001     1.619995     1.000000     0.008294     0.007418
75\%       0.580017     2.490005     1.000000     0.022695     0.011363
max      13.729996    22.960007     1.000000     0.195407     0.071223

            Ret\_15       Std\_15       Ret\_20       Std\_20       Ret\_25  {\ldots}  \textbackslash{}
count  2971.000000  2971.000000  2971.000000  2971.000000  2971.000000  {\ldots}
mean      0.007996     0.009473     0.010643     0.009577     0.013273  {\ldots}
std       0.040366     0.006703     0.045877     0.006543     0.050274  {\ldots}
min      -0.320822     0.001494    -0.370872     0.002007    -0.409051  {\ldots}
25\%      -0.007578     0.005288    -0.007721     0.005570    -0.006560  {\ldots}
50\%       0.013048     0.007591     0.016520     0.007800     0.019930  {\ldots}
75\%       0.028728     0.011410     0.034884     0.011496     0.040088  {\ldots}
max       0.241287     0.065627     0.212051     0.059167     0.248100  {\ldots}

           EMA\_190        lag\_1        lag\_2        lag\_3        lag\_4  \textbackslash{}
count  2971.000000  2971.000000  2971.000000  2971.000000  2971.000000
mean    163.806877   171.258269   171.175743   171.089990   171.003766
std      68.167202    71.091955    71.070362    71.040253    71.009352
min      62.545531    51.386787    51.386787    51.386787    51.386787
25\%     100.199986   105.710148   105.662731   105.651165   105.642242
50\%     164.247036   168.587280   168.572571   168.515594   168.472672
75\%     219.259284   231.435677   231.315384   231.196716   231.115906
max     304.055049   340.724152   340.724152   340.724152   340.724152

             lag\_5        lag\_6        lag\_7        lag\_8        lag\_9
count  2971.000000  2971.000000  2971.000000  2971.000000  2971.000000
mean    170.916197   170.828847   170.742130   170.654513   170.567015
std      70.972773    70.937136    70.902683    70.868631    70.835903
min      51.386787    51.386787    51.386787    51.386787    51.386787
25\%     105.641220   105.630589   105.619759   105.619560   105.594227
50\%     168.462601   168.411850   168.332382   168.284988   168.247406
75\%     231.058403   230.981613   230.868690   230.719635   230.638336
max     340.724152   340.724152   340.724152   340.724152   340.724152

[8 rows x 53 columns]
\end{Verbatim}
\end{tcolorbox}
        
    Some features exhibit significantly higher absolute values compared to the others. For these features, we will use the \texttt{MinMaxScaler()} method to scale them appropriately.

    \hypertarget{step-4-cleaning-data}{%
\subsection{STEP 4: Cleaning Data}\label{step-4-cleaning-data}}

    From our exploratory data analysis (EDA) process, we have identified
multicollinear features. We will develop a function to eliminate these
features and then implement it on our training data. Subsequently, we
will apply the same function to our test data.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{27}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} remove the first feature that is correlated with any other feature}
\PY{k}{def} \PY{n+nf}{correlated\PYZus{}features}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{threshold}\PY{o}{=}\PY{l+m+mf}{0.9}\PY{p}{)}\PY{p}{:}
    \PY{n}{col\PYZus{}corr} \PY{o}{=} \PY{n+nb}{set}\PY{p}{(}\PY{p}{)}
    \PY{n}{corr\PYZus{}matrix} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{corr\PYZus{}matrix}\PY{o}{.}\PY{n}{columns}\PY{p}{)}\PY{p}{)}\PY{p}{:}
        \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{i}\PY{p}{)}\PY{p}{:}
            \PY{k}{if} \PY{n+nb}{abs}\PY{p}{(}\PY{n}{corr\PYZus{}matrix}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{n}{i}\PY{p}{,} \PY{n}{j}\PY{p}{]}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{n}{threshold}\PY{p}{:}
                \PY{n}{colname} \PY{o}{=} \PY{n}{corr\PYZus{}matrix}\PY{o}{.}\PY{n}{columns}\PY{p}{[}\PY{n}{i}\PY{p}{]}
                \PY{n}{col\PYZus{}corr}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{colname}\PY{p}{)}
    \PY{k}{return} \PY{n}{col\PYZus{}corr}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{28}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Get the list of remaining features}
\PY{n}{drop\PYZus{}correlated\PYZus{}features} \PY{o}{=} \PY{n}{correlated\PYZus{}features}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{threshold}\PY{o}{=}\PY{l+m+mf}{0.9}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{29}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} drop the highly correlated features}
\PY{n}{X\PYZus{}train\PYZus{}drop} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{drop\PYZus{}correlated\PYZus{}features}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{X\PYZus{}train\PYZus{}drop}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{29}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                OC           HL         Sign       Ret\_10       Std\_10  \textbackslash{}
count  2971.000000  2971.000000  2971.000000  2971.000000  2971.000000
mean     -0.032289     2.180993     0.119488     0.005261     0.009328
std       1.653484     1.984382     0.991306     0.033085     0.006957
min     -11.680008     0.299995    -1.000000    -0.265117     0.001264
25\%      -0.750000     1.129997    -1.000000    -0.007946     0.005041
50\%      -0.110001     1.619995     1.000000     0.008294     0.007418
75\%       0.580017     2.490005     1.000000     0.022695     0.011363
max      13.729996    22.960007     1.000000     0.195407     0.071223

            Ret\_15       Ret\_20       Ret\_25       Ret\_30       SMA\_60
count  2971.000000  2971.000000  2971.000000  2971.000000  2971.000000
mean      0.007996     0.010643     0.013273     0.015933   168.784765
std       0.040366     0.045877     0.050274     0.053629    69.918583
min      -0.320822    -0.370872    -0.409051    -0.392927    60.405024
25\%      -0.007578    -0.007721    -0.006560    -0.005451   104.476129
50\%       0.013048     0.016520     0.019930     0.021935   169.387304
75\%       0.028728     0.034884     0.040088     0.044824   227.957799
max       0.241287     0.212051     0.248100     0.249708   323.832030
\end{Verbatim}
\end{tcolorbox}
        
    After removing most of the highly correlated features, it appears that
past returns, past volatility, SMA (Simple Moving Average), OC
(Open-Close), HL (High-Low), and Sign have significant predictive power.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{30}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X\PYZus{}test\PYZus{}drop} \PY{o}{=} \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{drop\PYZus{}correlated\PYZus{}features}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

\pagebreak
    \hypertarget{step-5-transformation}{%
\subsection{STEP 5: Transformation}\label{step-5-transformation}}

    We will visualize the scale of our data once more before proceeding with
feature transformation.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{31}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} study the distribution}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}
\PY{n}{sns}\PY{o}{.}\PY{n}{boxplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{variable}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{data}\PY{o}{=}\PY{n}{pd}\PY{o}{.}\PY{n}{melt}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}drop}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Boxplot of Features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{exam4_3_files/exam4_3_76_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The only remaining feature with a high value is \texttt{SMA\_60}. We
will scale this feature using \texttt{MinMaxScaler()}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{32}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{minmax} \PY{o}{=} \PY{n}{ColumnTransformer}\PY{p}{(}\PY{p}{[}
    \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{scaled}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{MinMaxScaler}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SMA\PYZus{}60}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\PY{p}{]}\PY{p}{,}\PY{n}{remainder} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{passthrough}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{33}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Fit and transform the data}
\PY{n}{sma\PYZus{}60} \PY{o}{=} \PY{n}{minmax}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}drop}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{34}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X\PYZus{}train\PYZus{}dropped\PYZus{}scaled} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}
    \PY{n}{sma\PYZus{}60}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{n}{minmax}\PY{o}{.}\PY{n}{get\PYZus{}feature\PYZus{}names\PYZus{}out}\PY{p}{(}\PY{p}{)}\PY{p}{,}
    \PY{n}{index}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}drop}\PY{o}{.}\PY{n}{index}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

\pagebreak
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{35}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X\PYZus{}train\PYZus{}dropped\PYZus{}scaled}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{35}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
       scaled\_\_SMA\_60  remainder\_\_OC  remainder\_\_HL  remainder\_\_Sign  \textbackslash{}
count     2971.000000    2971.000000    2971.000000      2971.000000
mean         0.411422      -0.032289       2.180993         0.119488
std          0.265419       1.653484       1.984382         0.991306
min          0.000000     -11.680008       0.299995        -1.000000
25\%          0.167299      -0.750000       1.129997        -1.000000
50\%          0.413710      -0.110001       1.619995         1.000000
75\%          0.636050       0.580017       2.490005         1.000000
max          1.000000      13.729996      22.960007         1.000000

       remainder\_\_Ret\_10  remainder\_\_Std\_10  remainder\_\_Ret\_15  \textbackslash{}
count        2971.000000        2971.000000        2971.000000
mean            0.005261           0.009328           0.007996
std             0.033085           0.006957           0.040366
min            -0.265117           0.001264          -0.320822
25\%            -0.007946           0.005041          -0.007578
50\%             0.008294           0.007418           0.013048
75\%             0.022695           0.011363           0.028728
max             0.195407           0.071223           0.241287

       remainder\_\_Ret\_20  remainder\_\_Ret\_25  remainder\_\_Ret\_30
count        2971.000000        2971.000000        2971.000000
mean            0.010643           0.013273           0.015933
std             0.045877           0.050274           0.053629
min            -0.370872          -0.409051          -0.392927
25\%            -0.007721          -0.006560          -0.005451
50\%             0.016520           0.019930           0.021935
75\%             0.034884           0.040088           0.044824
max             0.212051           0.248100           0.249708
\end{Verbatim}
\end{tcolorbox}

\pagebreak    
    Let's visualize our scaled data once more.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{36}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{18}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
\PY{n}{sns}\PY{o}{.}\PY{n}{boxplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{variable}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{data}\PY{o}{=}\PY{n}{pd}\PY{o}{.}\PY{n}{melt}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}dropped\PYZus{}scaled}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{After scaled}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Boxplot of Features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{exam4_3_files/exam4_3_83_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    It appears that the \texttt{OC} and \texttt{HL} columns contain a
substantial number of outliers. We will employ the \texttt{RobustScaler}
to transform these features.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{37}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{robust} \PY{o}{=} \PY{n}{ColumnTransformer}\PY{p}{(}\PY{p}{[}
        \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cols}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{RobustScaler}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{OC}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{HL}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
    \PY{p}{]}\PY{p}{,}\PY{n}{remainder} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{passthrough}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{oc\PYZus{}hl} \PY{o}{=} \PY{n}{robust}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}drop}\PY{p}{)}

\PY{n}{X\PYZus{}train\PYZus{}dropped\PYZus{}scaled} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}
    \PY{n}{oc\PYZus{}hl}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{n}{robust}\PY{o}{.}\PY{n}{get\PYZus{}feature\PYZus{}names\PYZus{}out}\PY{p}{(}\PY{p}{)}\PY{p}{,}
    \PY{n}{index}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}drop}\PY{o}{.}\PY{n}{index}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

\pagebreak
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{38}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{18}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
\PY{n}{sns}\PY{o}{.}\PY{n}{boxplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{variable}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{data}\PY{o}{=}\PY{n}{pd}\PY{o}{.}\PY{n}{melt}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}dropped\PYZus{}scaled}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{remainder\PYZus{}\PYZus{}SMA\PYZus{}60}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{After scaled}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Boxplot of Features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{exam4_3_files/exam4_3_86_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Now we can construct a preprocessing transformer that applies the
specified transformations to particular columns. We will fit and
transform the training data and subsequently transform the test data.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{39}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Instantiate transformer}
\PY{n}{preprocessing} \PY{o}{=} \PY{n}{ColumnTransformer}\PY{p}{(}\PY{p}{[}
        \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{MinMax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{MinMaxScaler}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SMA\PYZus{}60}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{,}
        \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Robust}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{RobustScaler}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{OC}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{HL}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{p}{]}
    \PY{p}{]}\PY{p}{,}\PY{n}{remainder} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{passthrough}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{40}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Fit and transform train set}
\PY{n}{train\PYZus{}transformed} \PY{o}{=} \PY{n}{preprocessing}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}drop}\PY{p}{)}
\PY{n}{X\PYZus{}train\PYZus{}transformed} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}
    \PY{n}{train\PYZus{}transformed}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{n}{preprocessing}\PY{o}{.}\PY{n}{get\PYZus{}feature\PYZus{}names\PYZus{}out}\PY{p}{(}\PY{p}{)}\PY{p}{,}
    \PY{n}{index}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}drop}\PY{o}{.}\PY{n}{index}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Transform test set}
\PY{n}{test\PYZus{}transformed} \PY{o}{=} \PY{n}{preprocessing}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}drop}\PY{p}{)}
\PY{n}{X\PYZus{}test\PYZus{}transformed} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}
    \PY{n}{test\PYZus{}transformed}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{n}{preprocessing}\PY{o}{.}\PY{n}{get\PYZus{}feature\PYZus{}names\PYZus{}out}\PY{p}{(}\PY{p}{)}\PY{p}{,}
    \PY{n}{index}\PY{o}{=}\PY{n}{X\PYZus{}test\PYZus{}drop}\PY{o}{.}\PY{n}{index}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

\pagebreak
    \hypertarget{step-6-modeling}{%
\subsection{STEP 6: Modeling}\label{step-6-modeling}}

    We will compare the default settings of some classifiers using the
cross-validation technique to identify potential candidates for our
final model. Additionally, we will use the \texttt{class\_weight}
parameter to address the previously identified class imbalance problem.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{41}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} cross\PYZhy{}validation}
\PY{n}{tscv} \PY{o}{=} \PY{n}{TimeSeriesSplit}\PY{p}{(}\PY{n}{n\PYZus{}splits}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{42}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} specify estimators}
\PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{42}
\PY{n}{dtc} \PY{o}{=} \PY{n}{DecisionTreeClassifier}\PY{p}{(}\PY{n}{class\PYZus{}weight}\PY{o}{=}\PY{n}{class\PYZus{}weight}\PY{p}{)}
\PY{n}{rfc} \PY{o}{=} \PY{n}{RandomForestClassifier}\PY{p}{(}\PY{n}{max\PYZus{}depth} \PY{o}{=} \PY{l+m+mi}{5} \PY{p}{,}\PY{n}{class\PYZus{}weight}\PY{o}{=}\PY{n}{class\PYZus{}weight}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}state}\PY{p}{)}
\PY{n}{knn} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{p}{)}
\PY{n}{gbc} \PY{o}{=} \PY{n}{GradientBoostingClassifier}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}state}\PY{p}{)}
\PY{n}{svc} \PY{o}{=} \PY{n}{SVC}\PY{p}{(}\PY{n}{class\PYZus{}weight}\PY{o}{=}\PY{n}{class\PYZus{}weight}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}state}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{43}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} get cv scores}
\PY{n}{clf} \PY{o}{=} \PY{p}{[}\PY{n}{dtc}\PY{p}{,} \PY{n}{rfc}\PY{p}{,} \PY{n}{knn}\PY{p}{,} \PY{n}{gbc}\PY{p}{,} \PY{n}{svc}\PY{p}{]}
\PY{k}{for} \PY{n}{estimator} \PY{o+ow}{in} \PY{n}{clf}\PY{p}{:}
    \PY{n}{score} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{estimator}\PY{p}{,} \PY{n}{X\PYZus{}train\PYZus{}transformed}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{scoring} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{n}{tscv}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The accuracy score of }\PY{l+s+si}{\PYZob{}}\PY{n}{estimator}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ is: }\PY{l+s+si}{\PYZob{}}\PY{n}{score}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{l+s+si}{:}\PY{l+s+s2}{0.4}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
The accuracy score of DecisionTreeClassifier(class\_weight=\{0: 2.435245901639344,
                                     1: 0.6291825497670478\}) is: 0.6505
The accuracy score of RandomForestClassifier(class\_weight=\{0: 2.435245901639344,
                                     1: 0.6291825497670478\},
                       max\_depth=5, random\_state=42) is: 0.6949
The accuracy score of KNeighborsClassifier() is: 0.7483
The accuracy score of GradientBoostingClassifier(random\_state=42) is: 0.5192
The accuracy score of SVC(class\_weight=\{0: 2.435245901639344, 1:
0.6291825497670478\}, random\_state=42) is: 0.5107
    \end{Verbatim}

    It appears that the \texttt{RandomForestClassifier()} and
\texttt{KNeighborsClassifier()} have the highest scores. Given that the
\texttt{KNeighborsClassifier()} may not perform well with imbalanced
classes, we will concentrate on building the model using the
\texttt{RandomForestClassifier()}.

    \hypertarget{base-model}{%
\subsubsection{Base Model}\label{base-model}}

    The default values for the parameters that determine the size of the
trees (e.g., \texttt{max\_depth}, \texttt{min\_samples\_leaf}, etc.)
result in fully grown and unpruned trees, which have the potential to
overfit our model. To address this, I will set \texttt{max\_depth} to 5
and then fine-tune this hyperparameter later.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{44}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{base\PYZus{}model} \PY{o}{=} \PY{n}{RandomForestClassifier}\PY{p}{(}\PY{n}{max\PYZus{}depth} \PY{o}{=} \PY{l+m+mi}{5}\PY{p}{,} \PY{n}{class\PYZus{}weight}\PY{o}{=}\PY{n}{class\PYZus{}weight}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}state}\PY{p}{)}
\PY{n}{base\PYZus{}model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}transformed}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
\PY{n+nb}{print} \PY{p}{(}\PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{252}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{base\PYZus{}model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}transformed}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{252}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
              precision    recall  f1-score   support

           0       0.48      0.88      0.62        67
           1       0.94      0.66      0.77       185

    accuracy                           0.72       252
   macro avg       0.71      0.77      0.70       252
weighted avg       0.82      0.72      0.73       252

    \end{Verbatim}

    \hypertarget{tuning-hyper-params}{%
\subsubsection{Tuning Hyper-params}\label{tuning-hyper-params}}

    We will obtain all the parameters and define our hyperparameter grid.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{45}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model} \PY{o}{=} \PY{n}{RandomForestClassifier}\PY{p}{(}\PY{n}{class\PYZus{}weight}\PY{o}{=}\PY{n}{class\PYZus{}weight}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}state}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{46}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model}\PY{o}{.}\PY{n}{get\PYZus{}params}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{46}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\{'bootstrap': True,
 'ccp\_alpha': 0.0,
 'class\_weight': \{0: 2.435245901639344, 1: 0.6291825497670478\},
 'criterion': 'gini',
 'max\_depth': None,
 'max\_features': 'sqrt',
 'max\_leaf\_nodes': None,
 'max\_samples': None,
 'min\_impurity\_decrease': 0.0,
 'min\_samples\_leaf': 1,
 'min\_samples\_split': 2,
 'min\_weight\_fraction\_leaf': 0.0,
 'n\_estimators': 100,
 'n\_jobs': -1,
 'oob\_score': False,
 'random\_state': 42,
 'verbose': 0,
 'warm\_start': False\}
\end{Verbatim}
\end{tcolorbox}

\pagebreak        
    As mentioned earlier, we will include \texttt{max\_depth},
\texttt{max\_leaf\_nodes}, and \texttt{n\_estimators} in our
hyperparameter grid for tuning to prevent overfitting. Additionally,
since we are dealing with an imbalanced classification problem, we will
experiment with different loss functions to determine their impact on
model performance during the hyperparameter search.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{47}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Hyper parameter optimization}
\PY{n}{param\PYZus{}grid} \PY{o}{=} \PY{p}{\PYZob{}}  \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{criterion}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gini}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{entropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{log\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}depth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{80}\PY{p}{,} \PY{l+m+mi}{90}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{110}\PY{p}{]}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{]}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min\PYZus{}samples\PYZus{}leaf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{]}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min\PYZus{}samples\PYZus{}split}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{12}\PY{p}{]}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{200}\PY{p}{,} \PY{l+m+mi}{300}\PY{p}{,} \PY{l+m+mi}{1000}\PY{p}{]}
            \PY{p}{\PYZcb{}}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{48}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} perform random search}
\PY{n}{gs} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{f1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{n}{tscv}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{gs}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}transformed}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{48}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
GridSearchCV(cv=TimeSeriesSplit(gap=0, max\_train\_size=None, n\_splits=5,
test\_size=None),
             estimator=RandomForestClassifier(class\_weight=\{0:
2.435245901639344,
                                                            1:
0.6291825497670478\},
                                              n\_jobs=-1, random\_state=42),
             n\_jobs=-1,
             param\_grid=\{'criterion': ['gini', 'entropy', 'log\_loss'],
                         'max\_depth': [80, 90, 100, 110],
                         'max\_features': [2, 3], 'min\_samples\_leaf': [3, 4, 5],
                         'min\_samples\_split': [8, 10, 12],
                         'n\_estimators': [100, 200, 300, 1000]\},
             scoring='f1')
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{49}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} best parameters}
\PY{n}{gs}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{49}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\{'criterion': 'entropy',
 'max\_depth': 80,
 'max\_features': 2,
 'min\_samples\_leaf': 3,
 'min\_samples\_split': 8,
 'n\_estimators': 1000\}
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{50}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} best score}
\PY{n}{gs}\PY{o}{.}\PY{n}{best\PYZus{}score\PYZus{}}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{50}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
0.8830577734916052
\end{Verbatim}
\end{tcolorbox}
        
    \hypertarget{step-7-metrics}{%
\subsection{STEP 7: Metrics}\label{step-7-metrics}}

    After fine-tuning our model and conducting a search for the best
hyperparameters, we will evaluate our model's performance and compare it
to our base model.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{51}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Refit the XGB Classifier with the best params}
\PY{n}{final\PYZus{}model} \PY{o}{=} \PY{n}{RandomForestClassifier}\PY{p}{(}\PY{n}{class\PYZus{}weight}\PY{o}{=}\PY{n}{class\PYZus{}weight}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}state}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{o}{*}\PY{o}{*}\PY{n}{gs}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}\PY{p}{)}
\PY{n}{final\PYZus{}model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}transformed}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{51}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
RandomForestClassifier(class\_weight=\{0: 2.435245901639344,
                                     1: 0.6291825497670478\},
                       criterion='entropy', max\_depth=80, max\_features=2,
                       min\_samples\_leaf=3, min\_samples\_split=8,
                       n\_estimators=1000, n\_jobs=-1, random\_state=42)
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{52}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Predicting the test dataset}
\PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{final\PYZus{}model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}transformed}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Measure Accuracy}
\PY{n}{acc\PYZus{}train} \PY{o}{=} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{final\PYZus{}model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}transformed}\PY{p}{)}\PY{p}{)}
\PY{n}{acc\PYZus{}test} \PY{o}{=} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Print Accuracy}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{ Training Accuracy }\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{: }\PY{l+s+si}{\PYZob{}}\PY{n}{acc\PYZus{}train}\PY{+w}{ }\PY{l+s+si}{:}\PY{l+s+s1}{0.4}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{ Test Accuracy }\PY{l+s+se}{\PYZbs{}t}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{: }\PY{l+s+si}{\PYZob{}}\PY{n}{acc\PYZus{}test}\PY{+w}{ }\PY{l+s+si}{:}\PY{l+s+s1}{0.4}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]

 Training Accuracy      : 0.9973
 Test Accuracy          : 0.6824
    \end{Verbatim}

    Our final model outperforms the base model, but it appears to suffer
from severe overfitting.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{53}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Cross validation score}
\PY{n}{score} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{final\PYZus{}model}\PY{p}{,}\PY{n}{X\PYZus{}train\PYZus{}transformed}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{n}{tscv}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Mean CV Score : }\PY{l+s+si}{\PYZob{}}\PY{n}{score}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{l+s+si}{:}\PY{l+s+s1}{0.4}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Mean CV Score : 0.7947
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{54}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Plot feature importance}
\PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
\PY{n}{feature\PYZus{}imp} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Importance Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{final\PYZus{}model}\PY{o}{.}\PY{n}{feature\PYZus{}importances\PYZus{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{X\PYZus{}train\PYZus{}transformed}\PY{o}{.}\PY{n}{columns}\PY{p}{\PYZcb{}}\PY{p}{)}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{n}{by}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Importance Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ascending}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\PY{n}{sns}\PY{o}{.}\PY{n}{barplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{n}{feature\PYZus{}imp}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Importance Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{n}{feature\PYZus{}imp}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Features Importance}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{exam4_3_files/exam4_3_114_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    It appears that the most important features are the asset returns,
volatilities, and H-L (High-Low) values. There is some predictive power
in the \texttt{SMA\_60} feature, while the \texttt{Sign} feature
contributes almost no predictive power.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{55}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Classification Report}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
              precision    recall  f1-score   support

           0       0.32      0.15      0.21       200
           1       0.74      0.88      0.80       543

    accuracy                           0.68       743
   macro avg       0.53      0.52      0.50       743
weighted avg       0.62      0.68      0.64       743

    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{56}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Display confussion matrix}
\PY{n}{disp} \PY{o}{=} \PY{n}{ConfusionMatrixDisplay}\PY{o}{.}\PY{n}{from\PYZus{}estimator}\PY{p}{(}
        \PY{n}{final\PYZus{}model}\PY{p}{,}
        \PY{n}{X\PYZus{}test\PYZus{}transformed}\PY{p}{,}
        \PY{n}{y\PYZus{}test}\PY{p}{,}
        \PY{n}{display\PYZus{}labels}\PY{o}{=}\PY{n}{final\PYZus{}model}\PY{o}{.}\PY{n}{classes\PYZus{}}\PY{p}{,}
        \PY{n}{cmap}\PY{o}{=}\PY{n}{plt}\PY{o}{.}\PY{n}{cm}\PY{o}{.}\PY{n}{Blues}
    \PY{p}{)}   
\PY{n}{disp}\PY{o}{.}\PY{n}{ax\PYZus{}}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Final Model}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{exam4_3_files/exam4_3_117_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Our model is performing well when predicting the majority class but
struggles when predicting the minority class.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{57}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Display ROCCurve}
\PY{n}{disp\PYZus{}roc} \PY{o}{=} \PY{n}{RocCurveDisplay}\PY{o}{.}\PY{n}{from\PYZus{}estimator}\PY{p}{(}
        \PY{n}{final\PYZus{}model}\PY{p}{,}
        \PY{n}{X\PYZus{}test\PYZus{}transformed}\PY{p}{,}
        \PY{n}{y\PYZus{}test}\PY{p}{,}
        \PY{n}{name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Tuned Random Forest}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{disp\PYZus{}roc}\PY{o}{.}\PY{n}{ax\PYZus{}}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ROC Curve}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{linestyle}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{exam4_3_files/exam4_3_119_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    When analyzing the ROC curve, it becomes evident that our model's
performance is only marginally better than random chance. This suggests
that the model may not effectively discriminate between positive and
negative outcomes. To enhance its predictive power and better address
the minority class, we may need to further refine our model or consider
additional strategies such as resampling techniques or employing
different algorithms.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{58}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Saving final model}
\PY{k+kn}{from} \PY{n+nn}{joblib} \PY{k+kn}{import} \PY{n}{dump}\PY{p}{,} \PY{n}{load}
\PY{n}{dump}\PY{p}{(}\PY{n}{clf}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{final\PYZus{}model.joblib}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} 
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{58}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
['final\_model.joblib']
\end{Verbatim}
\end{tcolorbox}

\pagebreak        
    \hypertarget{conclusion}{%
\subsection{Conclusion}\label{conclusion}}

We have successfully created a machine learning model capable of predicting upward and downward movements of the underlying asset using raw data obtained from Yahoo Finance APIs. Our process involved feature engineering to create a relevant feature set, feature selection to determine the most important features, and data transformation for our chosen set of features. The Random Forest Classifier is used as a candidate, fine-tuned, and optimize as our final model.

However, there are certain limitations to our model. The imbalanced nature of our labels, particularly in the context of financial time series, poses a challenge, which we've partly addressed using the \texttt{class\_weight} function. The daily price data is limited, providing relatively weak data structure for our algorithm; exploring higher frequency data may lead to a more robust training set. Additionally, our model exhibits overfitting, suggesting the potential need for a more complex model or better feature enginerring techniques.

    \hypertarget{references}{%
\section{References}\label{references}}

    {[}1{]}
\href{https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html}{Scikit-learn
1.3.2 - Voting Classifier}

{[}2{]} Mathematics for Machine Learning - Marc Peter Deisenroth - p.380

{[}3{]} Introduction to ML using Scikit-learn - Pythonlab 09


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
